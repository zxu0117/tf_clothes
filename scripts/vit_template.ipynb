{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60454906",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvit_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vit, utils\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtensorflow_addons\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "File \u001b[0;32m~/miniconda3/envs/vittf2.9_3.9plot/lib/python3.9/site-packages/vit_keras/vit.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtx\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, utils\n\u001b[1;32m      8\u001b[0m ConfigDict \u001b[38;5;241m=\u001b[39m tx\u001b[38;5;241m.\u001b[39mTypedDict(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfigDict\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     },\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m CONFIG_B: ConfigDict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlp_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3072\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m768\u001b[39m,\n\u001b[1;32m     25\u001b[0m }\n",
      "File \u001b[0;32m~/miniconda3/envs/vittf2.9_3.9plot/lib/python3.9/site-packages/vit_keras/utils.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msp\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from vit_keras import vit, utils\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import class_weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c7f55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'guardian_v2_weights', 'activation': 'relu', 'class_weights': 'True', 'num_epochs': 50, 'batch_size': 5}\n"
     ]
    }
   ],
   "source": [
    "# S3 Path to dataset s3://guardian-datasets/Guardian_curated_knock_grass_traintest.zip\n",
    "\n",
    "IMAGE_SIZE = 384\n",
    "\n",
    "dataset_loc = '/home/ubuntu/Data/Guardian_V2/'\n",
    "\n",
    "model_output_folder = \"/home/ubuntu/Models/inprogress/\"\n",
    "\n",
    "MODEL_PARAMETERS = {\"model_name\" : \"guardian_v2_weights\",                    \n",
    "                    \"activation\" : \"relu\", # can be relu / sigmoid\n",
    "                    \"class_weights\" : \"True\", # weight underrepresented\n",
    "                    \"num_epochs\" : 50,\n",
    "                    \"batch_size\" : 5}\n",
    "\n",
    "model_output_folder = model_output_folder+ MODEL_PARAMETERS[\"model_name\"] + \"/\"\n",
    "\n",
    "### Make model output folder if it doesnt exist\n",
    "\n",
    "if not os.path.isdir(model_output_folder):\n",
    "    os.mkdir(model_output_folder)\n",
    "\n",
    "# Write out parameters\n",
    "\n",
    "with open(model_output_folder+'MODEL_PARAMETERS.txt','w') as data: \n",
    "      data.write(str(MODEL_PARAMETERS))\n",
    "        \n",
    "print(MODEL_PARAMETERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8bff279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDataFrameForConditionImagesInFolders(dir_path):\n",
    "    # This logic creates a dataframe of image -> label used by keras\n",
    "    # cant use the simple image dataloader for regression tasks :(\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    print('loading images from ', dir_path)\n",
    "    train_classes = os.listdir(dir_path)\n",
    "\n",
    "    for int_class in train_classes:\n",
    "        int_images = os.listdir(dir_path + \"/\" + int_class + \"/\")\n",
    "\n",
    "        for img in int_images:\n",
    "            image_paths.append(dir_path + \"/\" + int_class + \"/\" + img)\n",
    "            labels.append(int_class)\n",
    "\n",
    "        print('Loaded images for', int_class, '\\t', len(int_images))\n",
    "\n",
    "    data = {'filepath': image_paths, 'target': labels}\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e55ca1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading images from  /home/ubuntu/Data/Guardian_V2/Train\n",
      "Loaded images for knock \t 907\n",
      "Loaded images for misc \t 1246\n",
      "Loaded images for aft \t 1154\n",
      "Loaded images for uncoveredpool \t 754\n",
      "Loaded images for debris \t 794\n",
      "Loaded images for bef \t 1084\n",
      "Loaded images for dur \t 1233\n",
      "loading images from  /home/ubuntu/Data/Guardian_V2/Test\n",
      "Loaded images for knock \t 100\n",
      "Loaded images for misc \t 100\n",
      "Loaded images for aft \t 100\n",
      "Loaded images for uncoveredpool \t 100\n",
      "Loaded images for debris \t 100\n",
      "Loaded images for bef \t 100\n",
      "Loaded images for dur \t 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'aft': 0.8878435256251548,\n",
       " 'bef': 0.9451765946230891,\n",
       " 'debris': 1.2903922274199353,\n",
       " 'dur': 0.8309581740238674,\n",
       " 'knock': 1.1296267128681683,\n",
       " 'misc': 0.8222884659481771,\n",
       " 'uncoveredpool': 1.3588480485032208}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = makeDataFrameForConditionImagesInFolders(dataset_loc + 'Train')\n",
    "\n",
    "'''\n",
    "train_df = train_df.sort_values(\"target\").reset_index()\n",
    "train_df[\"target_string\"] = train_df[\"target\"]\n",
    "train_df[\"target\"] = pd.Categorical(train_df[\"target\"])\n",
    "train_df[\"target\"] = train_df.target.cat.codes\n",
    "train_df[\"target\"] = pd.to_numeric(train_df[\"target\"])\n",
    "train_df[\"target\"] = str(train_df[\"target\"])\n",
    "'''\n",
    "\n",
    "train_df.to_csv(model_output_folder + \"train_df_guardian.csv\")\n",
    "\n",
    "val_df = makeDataFrameForConditionImagesInFolders(dataset_loc + 'Test')\n",
    "\n",
    "'''\n",
    "val_df = train_df.sort_values(\"target\").reset_index()\n",
    "val_df[\"target_string\"] = val_df[\"target\"]\n",
    "val_df[\"target\"] = pd.Categorical(val_df[\"target\"])\n",
    "val_df[\"target\"] = val_df.target.cat.codes\n",
    "val_df[\"target\"] = pd.to_numeric(val_df[\"target\"])\n",
    "val_df[\"target\"] = str(val_df[\"target\"])\n",
    "'''\n",
    "\n",
    "val_df.to_csv(model_output_folder + \"test_df_guardian.csv\")\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 classes = np.unique(train_df[\"target\"]),\n",
    "                                                 y = train_df[\"target\"])\n",
    "\n",
    "class_weights = dict(zip(np.unique(train_df[\"target\"]), class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f0ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentations, brightness, rotation etc.\n",
    "# Normalize RBG values 0> 1 : rescale=1./255,\n",
    "\n",
    "model = vit.vit_l32(\n",
    "    image_size=IMAGE_SIZE,\n",
    "    activation='sigmoid',\n",
    "    pretrained=True,\n",
    "    include_top=False,\n",
    "    pretrained_top=False,\n",
    ")\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        brightness_range=[0.5, 1.5],\n",
    "        rotation_range=.1,\n",
    "        width_shift_range=0.15,\n",
    "        height_shift_range=0.15,\n",
    "        rescale=1./255,\n",
    "        zoom_range=0.05,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip = False,\n",
    "        fill_mode='constant')\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dad173f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7172 validated image filenames belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Keras function just to get the formats correct for keras input\n",
    "train_generator=train_datagen.flow_from_dataframe(\n",
    "dataframe=train_df,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"target\",\n",
    "batch_size=MODEL_PARAMETERS['batch_size'],\n",
    "#has_ext=True,\n",
    "shuffle=True,\n",
    "class_mode=\"categorical\",\n",
    "target_size=(IMAGE_SIZE,IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8184673f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 700 validated image filenames belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Keras function just to get the formats correct for keras input\n",
    "\n",
    "\n",
    "val_generator=val_datagen.flow_from_dataframe(\n",
    "dataframe=val_df,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"target\",\n",
    "batch_size=MODEL_PARAMETERS['batch_size'],\n",
    "#has_ext=True,\n",
    "shuffle=False,\n",
    "class_mode=\"categorical\",\n",
    "target_size=(IMAGE_SIZE,IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76f00412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "x = model.output\n",
    "# x = layers.GlobalAveragePooling2D()(x) # TODO should \n",
    "predictions = layers.Dense(7)(x)\n",
    "new_model = Model(inputs=model.input, outputs=[predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8392bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Json describes the structure of the model, nodes / edges, encoding , normalization etc\n",
    "\n",
    "model_json = new_model.to_json()\n",
    "with open(model_output_folder + \"guardian_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8026cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monitoring status and writing out based on conditions such as best only true\n",
    "\n",
    "es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1, patience=.5*MODEL_PARAMETERS['num_epochs'])\n",
    "mc = ModelCheckpoint(\n",
    "    model_output_folder + \"guardian_model_efficientdet.{val_loss:.2f}.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode='min',\n",
    "    verbose=1, \n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    save_freq=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b98e414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each epoch, looks at all images\n",
    "# Batch means when it will update the loss\n",
    "\n",
    "nbatches_train, mod = divmod(train_df.shape[0], MODEL_PARAMETERS['batch_size'])\n",
    "STEP_SIZE_VALID=val_generator.n//val_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e4562f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(\n",
    "    optimizer=\"sgd\",\n",
    "    loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276ea955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  40/1434 [..............................] - ETA: 10:27 - loss: 7.5999"
     ]
    }
   ],
   "source": [
    "# Loss decreased, then climbed to a stable higher value\n",
    "# We will evaluate based on the \"test\" data\n",
    "\n",
    "new_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=STEP_SIZE_VALID,\n",
    "    steps_per_epoch=nbatches_train,\n",
    "    epochs=MODEL_PARAMETERS['num_epochs'],\n",
    "    workers=8,\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=[mc, es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f67fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vittf2.9_3.9plot",
   "language": "python",
   "name": "vittf2.9_3.9plot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
